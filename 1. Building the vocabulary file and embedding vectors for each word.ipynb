{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import csv\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracting text from xml files\n",
    "from xml.dom import minidom\n",
    "\n",
    "xmldoc = minidom.parse('1.1.text.xml')\n",
    "def getTokens(node):\n",
    "    tokens = []\n",
    "    if node is not None:\n",
    "        if node.nodeType == node.TEXT_NODE:\n",
    "            tokens.extend(node.data.split())\n",
    "        elif node.nodeType == node.ELEMENT_NODE:\n",
    "            for sub_array in [item.data.split() for item in node.childNodes]:\n",
    "                tokens.extend(sub_array)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5259"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#extracting all enities \n",
    "all_entities = []\n",
    "for entity in xmldoc.getElementsByTagName('entity'):\n",
    "    entityWords = getTokens(entity)\n",
    "    all_entities.append(entityWords)\n",
    "\n",
    "#print(all_entities)\n",
    "len(all_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def geTitle(nodelist):\n",
    "    titles = []\n",
    "    for node in nodelist:\n",
    "        if node.nodeType == node.TEXT_NODE:\n",
    "            titles.append(node.data)\n",
    "    return ''.join(titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "350"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#extract all titles. \n",
    "all_titles = []\n",
    "for title in xmldoc.getElementsByTagName('title'):\n",
    "    title_text = geTitle(title.childNodes)\n",
    "    all_titles.append(title_text.lower())\n",
    "#print(all_titles)  \n",
    "len(all_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def getAbstractText(abstractNode):\n",
    "    # Extract abstract text\n",
    "    abstractElements = []\n",
    "    for item in abstractNode.childNodes:\n",
    "        # If plain text node\n",
    "        if item.nodeType == item.TEXT_NODE:\n",
    "            abstractElements.append(item.data)\n",
    "        # If xml tag node\n",
    "        elif item.nodeType == item.ELEMENT_NODE:\n",
    "            for sub_array in [sub_item.data for sub_item in item.childNodes]:\n",
    "                abstractElements.append(sub_array)\n",
    "    return ''.join(abstractElements)\n",
    "\n",
    "\n",
    "all_abstracts = []\n",
    "for abstract in xmldoc.getElementsByTagName('abstract'):\n",
    "    abstractText = getAbstractText(abstract)\n",
    "    all_abstracts.append(abstractText)\n",
    "\n",
    "#print(all_abstracts)\n",
    "len(all_abstracts)\n",
    "type(all_abstracts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "700"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#350 titles + 350 abstracts\n",
    "all_data = all_titles + all_abstracts\n",
    "\n",
    "#print(all_data)\n",
    "len(all_data)\n",
    "#type(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43385"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tokenize text\n",
    "tokens = [word_tokenize(i) for i in all_data]\n",
    "tokens = list(itertools.chain.from_iterable(tokens))\n",
    "\n",
    "#print(tokens)\n",
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5469"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#remove stopwords, dublicated words and punctuation\n",
    "#stop_words = set(stopwords.words('english')) \n",
    "punctuation = ['.', ',', '//', ':', ';', ')', '(', '%', '-']\n",
    "#filtered_txt = [w for w in tokens if not w in stop_words]  \n",
    "filtered_txt = [] \n",
    "\n",
    "#for w in tokens: \n",
    "#    if w not in stop_words and w not in punctuation: \n",
    "#        filtered_txt.append(w)\n",
    "  \n",
    "for w in tokens: \n",
    "    if w not in punctuation: \n",
    "        filtered_txt.append(w)\n",
    "\n",
    "#remove dublicates\n",
    "tokens = list(set(tokens))        \n",
    "#print(tokens)\n",
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write vocabulary into a csv file\n",
    "with open('vocab.txt', 'w') as f:\n",
    "    for item in tokens:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5469"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#convert tokens of the vocabulary into a dataframe.\n",
    "df_vocabulary = pd.DataFrame(tokens)\n",
    "df_vocabulary.columns = ['Word']\n",
    "#print(df_vocabulary)\n",
    "df_vocabulary.head(5)\n",
    "len(df_vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetching Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get Global vectors and store them in the embeddings_dict\n",
    "embeddings_dict = {}\n",
    "f=open('C:/Users/CRS/Desktop/Thesis/Glove/glove.6B.300d.txt', encoding=\"utf8\")\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    vector = np.asarray(values[1:], \"float32\")\n",
    "    embeddings_dict[word] = vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-4.0088e-01,  3.8661e-01, -5.3597e-01, -8.1736e-02,  3.7877e-01,\n",
       "       -1.0792e-01, -1.6147e-01, -2.7286e-01,  1.2769e-01, -9.9632e-01,\n",
       "       -4.9670e-01, -2.0537e-02, -6.3191e-01, -7.4420e-02, -1.1382e-01,\n",
       "       -1.7873e-01, -2.3323e-01,  6.3002e-02,  3.7498e-01, -1.0195e+00,\n",
       "       -5.2802e-01, -4.8008e-01,  1.7030e-01,  9.8061e-01, -1.0713e-02,\n",
       "       -6.6502e-01,  1.8153e-02, -1.2372e+00, -1.8794e-01, -1.1094e-01,\n",
       "       -1.5612e-01, -7.6409e-02, -2.1182e-01, -1.9177e-01, -9.7702e-02,\n",
       "        1.3644e-01,  3.0259e-02, -2.0226e-01, -1.8277e-01, -9.2327e-02,\n",
       "        2.7776e-01, -3.5437e-01, -4.6328e-01,  2.4546e-02,  4.7235e-01,\n",
       "        4.1232e-01,  1.7078e-01,  1.6008e-01,  6.7490e-02,  3.9132e-01,\n",
       "       -4.1970e-01,  5.5769e-01,  1.2453e-04,  2.7068e-01, -4.2873e-01,\n",
       "       -1.6938e-01,  3.0346e-01, -3.3970e-01,  3.6246e-01,  1.1654e+00,\n",
       "       -2.2694e-01, -4.7843e-01,  7.3229e-01,  1.3514e-01, -1.2169e-01,\n",
       "       -2.5132e-01,  4.3905e-03,  2.9101e-01, -8.1070e-02,  5.9610e-01,\n",
       "        6.2256e-02, -5.6080e-01,  6.2352e-01,  3.2212e-01,  5.0335e-01,\n",
       "       -2.6570e-01,  5.6315e-01, -3.6209e-01,  2.6874e-01,  5.5533e-02,\n",
       "        7.0896e-02, -2.5238e-01,  4.5604e-01, -1.9588e-02, -1.0146e+00,\n",
       "        3.9054e-01,  1.3754e-01, -9.2939e-02, -5.9325e-01, -6.2414e-02,\n",
       "       -7.9319e-01, -1.6634e-01, -3.7344e-01,  6.6937e-01,  5.4510e-01,\n",
       "       -1.8163e-01, -1.1892e-01, -9.2854e-02, -5.0284e-02, -5.9577e-01,\n",
       "        5.1698e-01,  8.7663e-02,  1.5366e-01, -3.1056e-01,  3.4863e-02,\n",
       "       -2.5319e-01,  4.2562e-01, -2.7150e-01, -5.5312e-01,  3.6280e-01,\n",
       "        5.2728e-01,  1.5238e-01,  7.9542e-02,  1.4294e-01,  1.2760e-02,\n",
       "        4.6940e-01, -4.6580e-01,  5.1506e-01,  1.7120e-01,  5.3072e-01,\n",
       "       -1.8953e-01,  3.7155e-01, -1.2435e+00, -1.1691e-01, -4.5096e-01,\n",
       "        1.7546e-01, -4.3785e-01, -7.6426e-01,  4.8351e-01,  5.9331e-02,\n",
       "        3.3869e-01,  1.4959e-02,  5.5464e-01, -2.2781e-01, -4.1155e-01,\n",
       "       -2.4666e-02,  3.0573e-01,  1.8604e-01, -1.3718e-01,  6.1701e-01,\n",
       "        4.8771e-01, -6.6023e-01,  3.7421e-01, -4.8723e-01,  4.5354e-01,\n",
       "        2.3116e-01, -3.3630e-01,  3.7955e-03,  7.4306e-01,  4.3149e-01,\n",
       "        8.4256e-02,  1.2041e-01,  3.9835e-01,  3.4467e-01, -4.4860e-01,\n",
       "        3.8217e-01, -1.0821e-01,  1.9571e-03,  1.4581e-01, -3.1900e-02,\n",
       "        8.2914e-02, -2.9150e-01, -2.3519e-01,  1.3540e-01, -5.7247e-01,\n",
       "        4.9105e-01, -4.8130e-02,  1.1217e-01,  2.0138e-01, -1.6607e-01,\n",
       "        3.9208e-01,  2.6274e-01, -4.6218e-01,  8.3950e-01,  5.2344e-02,\n",
       "        2.3928e-02,  6.3089e-02,  7.3886e-01, -4.5296e-03, -7.9366e-01,\n",
       "       -6.3404e-01,  3.9386e-01,  2.5776e-02,  2.8181e-01, -6.0119e-01,\n",
       "       -4.0653e-01, -4.0625e-01,  1.5603e-01, -3.3502e-01, -8.3364e-02,\n",
       "       -8.4959e-02,  2.6993e-01, -1.1488e-01,  5.5651e-02, -2.9647e-01,\n",
       "       -2.8003e-01, -1.1289e-02,  7.8923e-02, -5.1776e-01, -1.2781e-01,\n",
       "       -4.1183e-01,  7.7814e-01,  4.2343e-02,  6.2818e-02, -7.5449e-01,\n",
       "        7.6105e-01, -5.7481e-01, -4.5044e-01,  5.6257e-01,  4.3610e-01,\n",
       "       -1.1859e-01, -1.5536e-02, -7.9300e-01,  4.8835e-01, -4.0965e-01,\n",
       "        3.6489e-02, -2.8344e-01, -8.3264e-02, -8.8039e-01, -4.0045e-01,\n",
       "       -2.7482e-01,  7.7821e-02, -8.2123e-02, -5.2717e-01, -2.0541e-01,\n",
       "        6.4063e-02, -2.7459e-01, -5.2568e-01,  5.7495e-01, -3.2570e-02,\n",
       "        1.4224e-01, -5.7503e-02, -3.6681e-01, -1.7525e-01, -3.5892e-01,\n",
       "       -2.0139e-02, -4.2594e-02, -9.7935e-01,  8.7179e-02,  1.2943e-01,\n",
       "       -1.7927e-01, -2.0214e-01,  5.5866e-01,  1.6439e-01, -9.0445e-01,\n",
       "        5.6119e-01, -5.6141e-01, -2.0995e-01,  2.1690e-01,  4.5711e-01,\n",
       "        1.2025e-01, -5.4430e-01, -4.3680e-01, -2.9891e-01, -4.6196e-01,\n",
       "        2.0936e-01,  3.8157e-01, -2.1178e-02, -9.8301e-03,  4.1075e-01,\n",
       "        1.5915e-01,  4.8436e-01,  2.5704e-01, -9.6869e-02, -8.4994e-02,\n",
       "        1.6553e-01, -5.9277e-02, -2.9945e-01,  1.7478e-02,  3.5349e-01,\n",
       "       -1.0130e-01,  9.3898e-02,  3.7414e-01, -1.5189e-01,  1.2960e-01,\n",
       "        1.5130e-01, -8.7611e-01,  4.8858e-02,  6.5550e-01, -2.1805e-01,\n",
       "        2.5969e-02, -6.1130e-01,  4.9472e-01,  8.2885e-01,  2.7615e-01,\n",
       "        4.0731e-01, -2.7541e-01, -2.4363e-01, -3.6306e-01,  5.4430e-01,\n",
       "       -5.9351e-01, -2.1667e-01, -1.2349e-01, -3.3741e-01,  3.0631e-01,\n",
       "       -3.4745e-01, -3.7423e-01, -2.1737e-01, -7.5832e-01, -1.8972e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_dict['oral'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400000\n"
     ]
    }
   ],
   "source": [
    "print(len(embeddings_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Word</th>\n",
       "      <th>Vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>the</td>\n",
       "      <td>[0.04656, 0.21318, -0.0074364, -0.45854, -0.03...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>,</td>\n",
       "      <td>[-0.25539, -0.25723, 0.13169, -0.042688, 0.218...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>.</td>\n",
       "      <td>[-0.12559, 0.01363, 0.10306, -0.10123, 0.09812...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>of</td>\n",
       "      <td>[-0.076947, -0.021211, 0.21271, -0.72232, -0.1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>to</td>\n",
       "      <td>[-0.25756, -0.057132, -0.6719, -0.38082, -0.36...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id Word                                             Vector\n",
       "0   0  the  [0.04656, 0.21318, -0.0074364, -0.45854, -0.03...\n",
       "1   1    ,  [-0.25539, -0.25723, 0.13169, -0.042688, 0.218...\n",
       "2   2    .  [-0.12559, 0.01363, 0.10306, -0.10123, 0.09812...\n",
       "3   3   of  [-0.076947, -0.021211, 0.21271, -0.72232, -0.1...\n",
       "4   4   to  [-0.25756, -0.057132, -0.6719, -0.38082, -0.36..."
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Convert dictionary into a DataFrame\n",
    "#put an id for each word\n",
    "embeddings_df = pd.DataFrame(list(embeddings_dict.items()), columns=['Word', 'Vector'])\n",
    "term_id = range(len(embeddings_dict))\n",
    "embeddings_df.insert(0, \"Id\", term_id, True) \n",
    "embeddings_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-4.0088e-01  3.8661e-01 -5.3597e-01 -8.1736e-02  3.7877e-01 -1.0792e-01\n",
      " -1.6147e-01 -2.7286e-01  1.2769e-01 -9.9632e-01 -4.9670e-01 -2.0537e-02\n",
      " -6.3191e-01 -7.4420e-02 -1.1382e-01 -1.7873e-01 -2.3323e-01  6.3002e-02\n",
      "  3.7498e-01 -1.0195e+00 -5.2802e-01 -4.8008e-01  1.7030e-01  9.8061e-01\n",
      " -1.0713e-02 -6.6502e-01  1.8153e-02 -1.2372e+00 -1.8794e-01 -1.1094e-01\n",
      " -1.5612e-01 -7.6409e-02 -2.1182e-01 -1.9177e-01 -9.7702e-02  1.3644e-01\n",
      "  3.0259e-02 -2.0226e-01 -1.8277e-01 -9.2327e-02  2.7776e-01 -3.5437e-01\n",
      " -4.6328e-01  2.4546e-02  4.7235e-01  4.1232e-01  1.7078e-01  1.6008e-01\n",
      "  6.7490e-02  3.9132e-01 -4.1970e-01  5.5769e-01  1.2453e-04  2.7068e-01\n",
      " -4.2873e-01 -1.6938e-01  3.0346e-01 -3.3970e-01  3.6246e-01  1.1654e+00\n",
      " -2.2694e-01 -4.7843e-01  7.3229e-01  1.3514e-01 -1.2169e-01 -2.5132e-01\n",
      "  4.3905e-03  2.9101e-01 -8.1070e-02  5.9610e-01  6.2256e-02 -5.6080e-01\n",
      "  6.2352e-01  3.2212e-01  5.0335e-01 -2.6570e-01  5.6315e-01 -3.6209e-01\n",
      "  2.6874e-01  5.5533e-02  7.0896e-02 -2.5238e-01  4.5604e-01 -1.9588e-02\n",
      " -1.0146e+00  3.9054e-01  1.3754e-01 -9.2939e-02 -5.9325e-01 -6.2414e-02\n",
      " -7.9319e-01 -1.6634e-01 -3.7344e-01  6.6937e-01  5.4510e-01 -1.8163e-01\n",
      " -1.1892e-01 -9.2854e-02 -5.0284e-02 -5.9577e-01  5.1698e-01  8.7663e-02\n",
      "  1.5366e-01 -3.1056e-01  3.4863e-02 -2.5319e-01  4.2562e-01 -2.7150e-01\n",
      " -5.5312e-01  3.6280e-01  5.2728e-01  1.5238e-01  7.9542e-02  1.4294e-01\n",
      "  1.2760e-02  4.6940e-01 -4.6580e-01  5.1506e-01  1.7120e-01  5.3072e-01\n",
      " -1.8953e-01  3.7155e-01 -1.2435e+00 -1.1691e-01 -4.5096e-01  1.7546e-01\n",
      " -4.3785e-01 -7.6426e-01  4.8351e-01  5.9331e-02  3.3869e-01  1.4959e-02\n",
      "  5.5464e-01 -2.2781e-01 -4.1155e-01 -2.4666e-02  3.0573e-01  1.8604e-01\n",
      " -1.3718e-01  6.1701e-01  4.8771e-01 -6.6023e-01  3.7421e-01 -4.8723e-01\n",
      "  4.5354e-01  2.3116e-01 -3.3630e-01  3.7955e-03  7.4306e-01  4.3149e-01\n",
      "  8.4256e-02  1.2041e-01  3.9835e-01  3.4467e-01 -4.4860e-01  3.8217e-01\n",
      " -1.0821e-01  1.9571e-03  1.4581e-01 -3.1900e-02  8.2914e-02 -2.9150e-01\n",
      " -2.3519e-01  1.3540e-01 -5.7247e-01  4.9105e-01 -4.8130e-02  1.1217e-01\n",
      "  2.0138e-01 -1.6607e-01  3.9208e-01  2.6274e-01 -4.6218e-01  8.3950e-01\n",
      "  5.2344e-02  2.3928e-02  6.3089e-02  7.3886e-01 -4.5296e-03 -7.9366e-01\n",
      " -6.3404e-01  3.9386e-01  2.5776e-02  2.8181e-01 -6.0119e-01 -4.0653e-01\n",
      " -4.0625e-01  1.5603e-01 -3.3502e-01 -8.3364e-02 -8.4959e-02  2.6993e-01\n",
      " -1.1488e-01  5.5651e-02 -2.9647e-01 -2.8003e-01 -1.1289e-02  7.8923e-02\n",
      " -5.1776e-01 -1.2781e-01 -4.1183e-01  7.7814e-01  4.2343e-02  6.2818e-02\n",
      " -7.5449e-01  7.6105e-01 -5.7481e-01 -4.5044e-01  5.6257e-01  4.3610e-01\n",
      " -1.1859e-01 -1.5536e-02 -7.9300e-01  4.8835e-01 -4.0965e-01  3.6489e-02\n",
      " -2.8344e-01 -8.3264e-02 -8.8039e-01 -4.0045e-01 -2.7482e-01  7.7821e-02\n",
      " -8.2123e-02 -5.2717e-01 -2.0541e-01  6.4063e-02 -2.7459e-01 -5.2568e-01\n",
      "  5.7495e-01 -3.2570e-02  1.4224e-01 -5.7503e-02 -3.6681e-01 -1.7525e-01\n",
      " -3.5892e-01 -2.0139e-02 -4.2594e-02 -9.7935e-01  8.7179e-02  1.2943e-01\n",
      " -1.7927e-01 -2.0214e-01  5.5866e-01  1.6439e-01 -9.0445e-01  5.6119e-01\n",
      " -5.6141e-01 -2.0995e-01  2.1690e-01  4.5711e-01  1.2025e-01 -5.4430e-01\n",
      " -4.3680e-01 -2.9891e-01 -4.6196e-01  2.0936e-01  3.8157e-01 -2.1178e-02\n",
      " -9.8301e-03  4.1075e-01  1.5915e-01  4.8436e-01  2.5704e-01 -9.6869e-02\n",
      " -8.4994e-02  1.6553e-01 -5.9277e-02 -2.9945e-01  1.7478e-02  3.5349e-01\n",
      " -1.0130e-01  9.3898e-02  3.7414e-01 -1.5189e-01  1.2960e-01  1.5130e-01\n",
      " -8.7611e-01  4.8858e-02  6.5550e-01 -2.1805e-01  2.5969e-02 -6.1130e-01\n",
      "  4.9472e-01  8.2885e-01  2.7615e-01  4.0731e-01 -2.7541e-01 -2.4363e-01\n",
      " -3.6306e-01  5.4430e-01 -5.9351e-01 -2.1667e-01 -1.2349e-01 -3.3741e-01\n",
      "  3.0631e-01 -3.4745e-01 -3.7423e-01 -2.1737e-01 -7.5832e-01 -1.8972e-01]\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(embeddings_df)):\n",
    "    if embeddings_df['Word'][i] == 'oral':\n",
    "        print(embeddings_df['Vector'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4011"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get glove word embeddings for the words in our vocabulary. \n",
    "# constract a dictionary which contains words in our vocabulary and their respective embedding from GloVe. \n",
    "vocab_dictionary = {}\n",
    "vocab_words = []\n",
    "test_words = []\n",
    "vector_embeddings = []\n",
    "for i in range (len(df_vocabulary)):\n",
    "    if df_vocabulary['Word'][i] in embeddings_dict:\n",
    "        vocab_words.append(df_vocabulary['Word'][i])\n",
    "        vector_embeddings.append(embeddings_dict[df_vocabulary['Word'][i]])\n",
    "\n",
    "#print(vocab_words)\n",
    "#print(vector_embeddings)\n",
    "len(vocab_words)\n",
    "#type(vector_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Embedding Vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>but</td>\n",
       "      <td>[-0.0093601, 0.22789, -0.10275, 0.0010893, 0.2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>reported</td>\n",
       "      <td>[-0.21093, 0.51757, 0.042662, 0.013656, -0.577...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>groups</td>\n",
       "      <td>[-0.12724, 0.27224, -0.055019, -0.0030393, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>clustering</td>\n",
       "      <td>[-0.49152, 0.83904, 0.48919, -0.53234, -0.0898...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bootstrap</td>\n",
       "      <td>[-0.54975, 0.13891, 0.58945, -0.24693, 0.30243...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Word                                   Embedding Vector\n",
       "0         but  [-0.0093601, 0.22789, -0.10275, 0.0010893, 0.2...\n",
       "1    reported  [-0.21093, 0.51757, 0.042662, 0.013656, -0.577...\n",
       "2      groups  [-0.12724, 0.27224, -0.055019, -0.0030393, -0....\n",
       "3  clustering  [-0.49152, 0.83904, 0.48919, -0.53234, -0.0898...\n",
       "4   bootstrap  [-0.54975, 0.13891, 0.58945, -0.24693, 0.30243..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "4011"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#word_id = range(len(df_vocabulary))\n",
    "word_vector_final= pd.DataFrame({'Word': vocab_words, 'Embedding Vector': vector_embeddings})     \n",
    "word_vector_final.index = range(len(word_vector_final)) \n",
    "display(word_vector_final.head(5))\n",
    "len(word_vector_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4011"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#merge the words in your vocabulary file with the ID and embeddings from glove. \n",
    "#get word embedding of the words in the vocabulary by simply joininig the dataframes. \n",
    "vocabulary = pd.merge(df_vocabulary, embeddings_df, on='Word')\n",
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#vocabulary['Vector'][0]\n",
    "len(vocabulary['Vector'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Word</th>\n",
       "      <th>Vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>34</td>\n",
       "      <td>but</td>\n",
       "      <td>[-0.0093601, 0.22789, -0.10275, 0.0010893, 0.2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>293</td>\n",
       "      <td>reported</td>\n",
       "      <td>[-0.21093, 0.51757, 0.042662, 0.013656, -0.577...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>503</td>\n",
       "      <td>groups</td>\n",
       "      <td>[-0.12724, 0.27224, -0.055019, -0.0030393, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>45209</td>\n",
       "      <td>clustering</td>\n",
       "      <td>[-0.49152, 0.83904, 0.48919, -0.53234, -0.0898...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>79856</td>\n",
       "      <td>bootstrap</td>\n",
       "      <td>[-0.54975, 0.13891, 0.58945, -0.24693, 0.30243...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4640</td>\n",
       "      <td>valuable</td>\n",
       "      <td>[0.34956, 0.24624, 0.20949, -0.15911, 0.3061, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4968</td>\n",
       "      <td>preferred</td>\n",
       "      <td>[-0.20157, 0.31404, 0.42666, -0.17512, 0.39641...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>136385</td>\n",
       "      <td>unordered</td>\n",
       "      <td>[0.26952, 0.00056443, 0.086649, 0.37182, -0.04...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>35890</td>\n",
       "      <td>'95</td>\n",
       "      <td>[-0.3863, 0.23094, -0.38055, -0.28722, 0.15539...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>23821</td>\n",
       "      <td>cf</td>\n",
       "      <td>[0.053901, 0.64365, 0.046508, 0.037297, -0.183...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Id        Word                                             Vector\n",
       "0      34         but  [-0.0093601, 0.22789, -0.10275, 0.0010893, 0.2...\n",
       "1     293    reported  [-0.21093, 0.51757, 0.042662, 0.013656, -0.577...\n",
       "2     503      groups  [-0.12724, 0.27224, -0.055019, -0.0030393, -0....\n",
       "3   45209  clustering  [-0.49152, 0.83904, 0.48919, -0.53234, -0.0898...\n",
       "4   79856   bootstrap  [-0.54975, 0.13891, 0.58945, -0.24693, 0.30243...\n",
       "5    4640    valuable  [0.34956, 0.24624, 0.20949, -0.15911, 0.3061, ...\n",
       "6    4968   preferred  [-0.20157, 0.31404, 0.42666, -0.17512, 0.39641...\n",
       "7  136385   unordered  [0.26952, 0.00056443, 0.086649, 0.37182, -0.04...\n",
       "8   35890         '95  [-0.3863, 0.23094, -0.38055, -0.28722, 0.15539...\n",
       "9   23821          cf  [0.053901, 0.64365, 0.046508, 0.037297, -0.183..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary = vocabulary[['Id','Word', 'Vector']]\n",
    "display(vocabulary.head(10))\n",
    "type(vocabulary['Vector'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "vocabulary.to_pickle(\"GloVe.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_pickle('GloVe.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(len(vocabulary.values))\n",
    "file2write=open(\"GloVe.txt\",'w')\n",
    "file2write.write(str(vocabulary.values))\n",
    "file2write=open(\"GloVe.txt\",\"r+\") \n",
    "#file2write.close()\n",
    "#file2write.read()\n",
    "#print(file2write)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>34</td>\n",
       "      <td>but</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>293</td>\n",
       "      <td>reported</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>503</td>\n",
       "      <td>groups</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>45209</td>\n",
       "      <td>clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>79856</td>\n",
       "      <td>bootstrap</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Id        Word\n",
       "0     34         but\n",
       "1    293    reported\n",
       "2    503      groups\n",
       "3  45209  clustering\n",
       "4  79856   bootstrap"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vocab = vocabulary.drop(columns=['Vector'])\n",
    "display(vocab.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab.to_csv('vocab.csv', index=False) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
